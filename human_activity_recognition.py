# -*- coding: utf-8 -*-
"""human_activity recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B5VdaIV5KCYQR2vSdEswDKo7bzx_vy8v

# 1. Importing Required Libraries
# We begin by importing all necessary Python packages. These tools cover everything from data manipulation (`pandas`, `numpy`) to machine learning (`sklearn`) and visualization (`matplotlib`, `seaborn`).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib # Used for saving/loading the trained model
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

""" 2. Configuration and Setup
# Setting a random seed ensures that our results are consistent and reproducible every time the notebook is run.
"""

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# Define file names for the datasets and the output model file
TRAIN_FILE = 'train.csv'
TEST_FILE = 'test.csv'
MODEL_FILENAME = 'random_forest_har_model.joblib'

# Initialize key variables to None/empty arrays for safe execution checks
df_train, df_test = None, None
X_train, X_test, y_train, y_test = None, None, None, None
y_pred = np.array([])
activity_labels = []
model = None

"""3. Loading the Datasets
# We load the `train.csv` and `test.csv` files into Pandas DataFrames. Ensure these files have been successfully uploaded to your Colab session storage before running this cell.
"""

print(f"Loading {TRAIN_FILE} and {TEST_FILE}...")
try:
    df_train = pd.read_csv(TRAIN_FILE)
    df_test = pd.read_csv(TEST_FILE)
    print("Data successfully loaded!")
except FileNotFoundError:
    print("Error: train.csv or test.csv not found. Please upload the files.")
    # df_train and df_test remain None here

"""# 4. Initial Data Inspection
# Let's quickly inspect the structure of the training data. The data contains 561 features and two additional columns, `subject` and `Activity`.
"""

print("\n--- Training Data Info ---")
if df_train is not None:
    print(f"Train Shape: {df_train.shape}")
    print(f"Number of features: {df_train.shape[1] - 2}")
    print("\nFirst 5 rows of the dataset:")
    print(df_train.head())

# Check for missing data
print("\n--- Missing Values Check ---")
if df_train is not None and df_test is not None:
    print(f"Train missing values count: {df_train.isnull().sum().sum()}")
    print(f"Test missing values count: {df_test.isnull().sum().sum()}")
elif df_train is None:
    print("Cannot check missing values: Training data is not loaded.")
elif df_test is None:
    print("Cannot check missing values: Test data is not loaded.")

"""5. Separating Features (X) and Labels (y)
# We split the data into feature matrices (`X`) and target vectors (`y`). The last two columns are the non-feature data we must exclude.

"""

if df_train is not None and df_test is not None:
    # Features (X): All columns except the last two
    X_train = df_train.iloc[:, :-2]
    y_train_raw = df_train.iloc[:, -1] # Target label (Activity)

    X_test = df_test.iloc[:, :-2]
    y_test_raw = df_test.iloc[:, -1]

    print(f"X_train shape: {X_train.shape}, y_train shape: {y_train_raw.shape}")
    print(f"X_test shape: {X_test.shape}, y_test shape: {y_test_raw.shape}")
else:
    print("Skipping feature separation: DataFrames are not loaded.")
    # The default None values for X/y will prevent subsequent cells from running.

"""6. Checking Activity Distribution
# Good practice dictates checking the balance of our target classes. The UCI HAR dataset is typically well-balanced.

"""

if 'y_train_raw' in locals() and y_train_raw is not None:
    print("\n--- Activity Distribution in Training Set ---")
    print(y_train_raw.value_counts())
else:
    print("\nSkipping activity distribution check: Raw labels (y_train_raw) not available.")

"""# 7. Encoding Activity Labels
# Machine learning algorithms only understand numbers. We use `LabelEncoder` to convert the activity names (strings) into unique integer IDs.
"""

if 'y_train_raw' in locals() and y_train_raw is not None:
    label_encoder = LabelEncoder()

    # --- FIX for 'float has no len()' error ---
    # Explicitly convert raw labels to string type to handle any potential NaN/float issues in the Activity column
    y_train_str = y_train_raw.astype(str)
    y_test_str = y_test_raw.astype(str)

    # Fit the encoder on all unique labels (now strings)
    all_labels = pd.concat([y_train_str, y_test_str]).unique()
    label_encoder.fit(all_labels)

    y_train = label_encoder.transform(y_train_str)
    y_test = label_encoder.transform(y_test_str)
    # -------------------------------------------

    # Store the original activity names for reporting later
    activity_labels = label_encoder.classes_
    print(f"\nActivities were mapped to numerical IDs: {list(activity_labels)}")
else:
    print("\nSkipping label encoding: Raw labels not available.")

"""# 8. Model Initialization and Training

# We initialize our Random Forest model and fit it to the training data. The parameters are chosen for good baseline performance on this complex dataset.

"""

if X_train is not None and y_train is not None:
    print("\n--- Training Random Forest Classifier ---")

    # Model configuration: 100 trees, max depth 10 to limit complexity
    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)
    print("Training complete!")
else:
    print("\nSkipping model training: Training features (X_train) or labels (y_train) are missing.")
    # 'model' remains None

""" 9. Saving the Trained Model

# For deployment (e.g., to Hugging Face or a web application), we need to save the model. We use `joblib` for efficient saving of Scikit-learn models.
"""

if model is not None:
    joblib.dump(model, MODEL_FILENAME)
    print(f"\nModel successfully saved as {MODEL_FILENAME}. Download this file for deployment!")
else:
    print("\nSkipping model saving: Model was not trained.")

"""# 10. Model Prediction and Accuracy Score

# Now we use the trained model to predict activities on the unseen test set and calculate the overall accuracy.
"""

# Make predictions on the test set (X_test)
if model is not None and X_test is not None:
    y_pred = model.predict(X_test)
    print("\nPredictions successfully generated.")
else:
    print("Error: Model or test data (X_test) not defined. Cannot make predictions.")
    # y_pred remains an empty array

# Calculate Overall Accuracy Score
if y_pred.size > 0:
    accuracy = accuracy_score(y_test, y_pred)
    print("\n--- Model Evaluation Results ---")
    print(f"Overall Accuracy Score: {accuracy * 100:.2f}%")
else:
    print("\n--- Model Evaluation Results ---")
    print("Skipping accuracy calculation due to prediction failure.")

"""# 11. Detailed Performance Evaluation: Classification Report

# The classification report gives a much deeper view than just accuracy, showing Precision, Recall, and F1-score for *each* activity class.

"""

print("\n--- Detailed Classification Report ---")
if y_pred.size > 0 and len(activity_labels) > 0:
    print(classification_report(y_test, y_pred, target_names=activity_labels))
else:
    print("Skipping classification report because predictions (y_pred) are empty or labels are missing.")

""" 12. Detailed Performance Evaluation: Confusion Matrix

# A visual representation of the confusion matrix helps us quickly identify which activities the model confuses. Ideally, all high numbers should be on the diagonal.

"""

if y_pred.size > 0 and len(activity_labels) > 0:
    conf_matrix = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(10, 8))
    sns.heatmap(
        conf_matrix,
        annot=True,
        fmt='d', # Integer format
        cmap='Blues',
        xticklabels=activity_labels,
        yticklabels=activity_labels
    )
    plt.title('Confusion Matrix for Human Activity Recognition')
    plt.ylabel('True Activity')
    plt.xlabel('Predicted Activity')
    plt.show()
else:
    print("Skipping confusion matrix visualization because predictions (y_pred) are empty or labels are missing.")

# ---

"""# 13. Feature Importance Analysis

# What sensor data drives the classification? We plot the top 10 most influential features according to the Random Forest model.
"""

# Extract feature importances
if model is not None and X_train is not None:
    feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)

    # Get the top 10 most important features
    top_10_features = feature_importances.nlargest(10)

    plt.figure(figsize=(12, 6))
    top_10_features.plot(kind='barh', color='#388E8E')
    plt.title('Top 10 Most Important Features for HAR Classification')
    plt.xlabel('Feature Importance Score (Random Forest)')
    plt.ylabel('Feature Name')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
else:
    print("Skipping feature importance analysis because the model or training data is missing.")

""" # 14. Conclusion

# The project successfully built a Human Activity Recognition model with high accuracy. This model, saved as `random_forest_har_model.joblib`, is ready for use in deployment environments.
"""